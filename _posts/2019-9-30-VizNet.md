---
layout: post
title: (CHI2019) VizNet, Towards A Large-Scale Visualization Learning and Benchmarking Repository
---

Researchers currently rely on ad hoc datasets to train automated visualization tools and evaluate the efectiveness of visualization designs. These exemplars often lack the characteristics of real-world datasets, and their one-of nature makes it difcult to compare diferent techniques. This paper presents VizNet: a large-scale corpus of over 31 million datasets (657GB of data) compiled from open data repositories and online visualization galleries. 

On average, these datasets comprise 17 records describing 3 dimensions of data. 51% of the dimensions record categorical data, 44% quantitative, and only 5% measure temporal information. VizNet provides the necessary common baseline for comparing visualization design techniques,and developing benchmark models and algorithms for automating visual analysis.

VizNet along with data collection and analysis scripts is publicly available at <https://viznet.media.mit.edu>.

## Methods
VizNet incorporates four large-scale corpora, assembled from the web, online visualization tools, and open data portals.
1. horizontal relational tables from the WebTables 2015 corpus, which extracts structured tables from the Common Crawl
2. tabular data uploaded by users of two popular online data visualization and analysis systems, i.e., Plotly and ManyEyes
3. public data from the Open Data Portal Watch, which catalogs and monitors 262 open data portals, the majority of which are hosted by governments, and collect civic and social data

VizNet aggregates these corpora into a centralized repository. However, the majority of datasets are from WebTables. Therefore, 250K randomly sampled datasets are obtained from each corpus to avoid oversampling the WebTable corpus. These datasets are combined into a balanced sample of one million datasets, which is referred to as the **VizNet 1M corpus**.

## Summary statistics and distributions of VizNet
Figure below shows summary statistics and underlying distributions for each of the four source corpora and the VizNet 1M corpus. The data type of a column is classifed as either categorical, quantitative, or temporal, which are abbreviated as C, Q and T, respectively. This data type is detected using a heuristic-based approach that incorporates column name and value information. 

For quantitative columns, the Kolmogorov-Smirnov test is used to examine the goodness-offt of six distributions: the normal, log-normal, exponential, power law, uniform and chi-squared distributions. The null hypothesis of a distribution fit is rejected if the p-value of the associated test is lower than the level α = 0.05. If all distributions are rejected at α, the distribution is considered to be undefned. If multiple distributions are not rejected, the “best" fit is considered to be that with the highest p-value. 

Also reported are the skewness and percent of outliers, defned as data points that fall more than 1.5 × IQR below the frst quartile or above the third quartile, where IQR is the interquartile range. The statistical distribution of categorical columns within each corpus is characterized using the normalized entropy.

![_config.yml]({{ site.baseurl }}/images/VizNet1.png)

![_config.yml]({{ site.baseurl }}/images/VizNet2.PNG)

## Experimentation
The paper also presents three experiments to evaluate the utility of VizNet as a resource for data scientists and visualization researchers. The Kim and Heer (2018) prior study is replicated using real-world datasets from the VizNet corpus to assess the infuence of user task and data distribution on visual encoding efectiveness. This experiment is then extended by including an additional task on outlier detection. Finally, a machine learning model is trained to learn the perceptual efectiveness of diferent visual designs and the model's predictive power across test datasets is evaluated. 

Please refer to the paper for experiment detals and results.
