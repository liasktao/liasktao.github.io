---
layout: post
title: (CHI2019) VizML, A Machine Learning Approach to Visualization Recommendation
---

## Problem statment and background
Visualization recommender systems aim to lower the barrier to exploring basic visualizations.
Most recommender systems encode visualization guidelines as collection of “if-then” statements, or rules, to automatically generate visualizations for analysts to search and select, rather than manually specify. Examples include APT, SAGE, BOZ, Show Me, Voyager, Explore in Google Sheets, VizDeck, and DIVE, all suffering from three major limitations:
1. visualization is a complex process that may require modelling non-linear relationships that are difcult to capture with simple rules
2. crafting rule sets is a costly process that relies on expert judgment
3. as the dimension of input data increases, the combinatorial nature of rules result in an explosion of possible recommendations

In contrast, machine learning (ML)-based systems directly learn the relationship between data and visualizations by training models on analyst interaction.
Existing systems include 
* **DeepEye** classifies a visualization as “good” or “bad” and 2) rank lists of visualizations
* **Data2Vis** creates a sequence-to-sequence model that maps JSON-encoded datasets to Vega-lite visualization specifcations
* **Draco-Learn** learns trade-offs between constraints

However, these systems do not learn to make visualization design choices as an analyst would, which impacts interpretability and ease of integration into existing systems. 
Furthermore, because these systems are trained with annotations on rule-generated visualizations in controlled settings, they are limited by the quantity and quality of data. Specifically,


## Methods
Next you can update your site name, avatar and other options using the _config.yml file in the root of your repository (shown below).

![_config.yml]({{ site.baseurl }}/images/VizML.png)

Models used in evaluation
* neural networks (primary)
* naive Bayes
* K-nearest neighbors
* logistic regression
* random forest
For all models, the data is split into 60/20/20 train/validation/test sets and 5-fold cross-validation is used.

Code and datasets used in the paper are available at: [https://github.com/mitmedialab/vizml](https://github.com/mitmedialab/vizml)

## Evaluation
The neural network consistently outperforms the baseline models and model performance generally progresses as NB< KNN < LR ≈ RF < NN.

Benchmarking with crowdsourced effectiveness

![_config.yml]({{ site.baseurl }}/images/VizML2.png)

## Discussion
Limitations:
* VizML is biased towards the Plotly dataset
* Plotly and crowdsource users are not experts in data visualization
* The paper was only focused on a subset of the tasks in a visualization recommendation pipeline

Future work:
* a need for more diverse training data from other tools (e.g., Many Eyes and Tableau)
* objective measures of visualization effectiveness
